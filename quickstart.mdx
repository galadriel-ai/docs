This quickstart covers two things:

1. Downloading & starting the node client
2. CLI commands to use client

### Prerequisites

* [Requirements](/requirements)

### Run the LLM
<Steps>
  <Step title="Run the LLM">
    Create a python environment
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
  </Step>
  <Step title="Install vllm">
    ```bash
    pip install vllm==0.5.5
    ```
  </Step>
  <Step title="Run vllm">
    Replace `<HUGGING_FACE_TOKEN>` with your [huggingface](https://huggingface.co/) access token
    ```bash
    HUGGING_FACE_HUB_TOKEN=<HUGGING_FACE_TOKEN> \
      nohup vllm serve neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8 \
      --revision 3aed33c3d2bfa212a137f6c855d79b5426862b24 \
      --max-model-len 16384 \
      --gpu-memory-utilization 1 \
      --host localhost \
      --disable-frontend-multiprocessing \
      --port 11434 > logs_llm.log 2>&1 &
      ```
  </Step>
  <Step title="Ensure vllm works">
    Check the logs
    ```bash
    tail -f logs_llm.log
    ```
    Once you see the API is working try calling it
    ```bash
    curl http://localhost:11434/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: a" \
    -d '{
        "model": "neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8",
        "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant."
        },
        {
          "role": "user",
          "content": "Hi, whats up?"
        }
      ],
    }'
    ```
  </Step>
</Steps>

### Run a GPU node from the command line

<Steps>
  <Step title="Run the LLM">
    Create a (separate) python environment
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

  </Step>
  <Step title="Install galadriel">
    ```bash
    pip install galadriel-node
    ```
  </Step>
  <Step title="Setup environment">
    TODO: env setup command
  </Step>
  <Step title="Run the node in the background">
    ```bash
    nohup galadriel node run > logs.log 2>&1 &
    ```
    If this is your first time running the GPU node, it will perform hardware validation
    and LLM benchmarking, to ensure your setup is working correctly and is fast enough.
  </Step>
  <Step title="Check the node status">
    Check the logs
    ```bash
    tail -f logs.log
    ```
    Check the node status
    ```bash
    galadriel node status
    ```
    if the status is <span style={{color: "green"}}>online</span> you've successfully started your GPU node.

  </Step>

</Steps>

### Checking node metrics

Prerequisites:
* You've deployed the GPU node and its status is <span style={{ color: "green" }}>online</span>.

To check the node stats
```bash
galadriel node stats
```


What's next?

* Keep the node running and earn [points](/incentives).
